{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Burgers Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "utils_path = Path('../utils').resolve()\n",
    "sys.path.insert(0, str(utils_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.io\n",
    "\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "from graphs_utils import newfig, savefig\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "print(f'Using {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(DNN, self).__init__()\n",
    "        \n",
    "        # parameters\n",
    "        self.depth = len(layers) - 1\n",
    "        \n",
    "        # set up layer order dict\n",
    "        self.activation = torch.nn.Tanh\n",
    "        \n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1): \n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
    "            )\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "            \n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        \n",
    "        # deploy layers\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsInformedNN():\n",
    "    def __init__(self, X, u, layers, lb, ub):\n",
    "        \n",
    "        # boundary conditions\n",
    "        self.lb = torch.tensor(lb).float().to(device)\n",
    "        self.ub = torch.tensor(ub).float().to(device)\n",
    "        \n",
    "        # data\n",
    "        self.x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device)\n",
    "        self.t = torch.tensor(X[:, 1:2], requires_grad=True).float().to(device)\n",
    "        self.u = torch.tensor(u).float().to(device)\n",
    "        \n",
    "        # settings\n",
    "        self.lambda_1 = torch.tensor([0.0], requires_grad=True).to(device)\n",
    "        self.lambda_2 = torch.tensor([-6.0], requires_grad=True).to(device)\n",
    "        \n",
    "        self.lambda_1 = torch.nn.Parameter(self.lambda_1)\n",
    "        self.lambda_2 = torch.nn.Parameter(self.lambda_2)\n",
    "        \n",
    "        # deep neural networks\n",
    "        self.dnn = DNN(layers).to(device)\n",
    "        self.dnn.register_parameter('lambda_1', self.lambda_1)\n",
    "        self.dnn.register_parameter('lambda_2', self.lambda_2)\n",
    "        \n",
    "         # optimizers: using the same settings\n",
    "        self.optimizer = torch.optim.LBFGS(\n",
    "            self.dnn.parameters(), \n",
    "            lr=1.0, \n",
    "            max_iter=50000, \n",
    "            max_eval=50000, \n",
    "            history_size=50,\n",
    "            tolerance_grad=1e-5, \n",
    "            tolerance_change=1.0 * np.finfo(float).eps,\n",
    "            line_search_fn=\"strong_wolfe\"       # can be \"strong_wolfe\"\n",
    "        )\n",
    "        \n",
    "        self.optimizer_Adam = torch.optim.Adam(self.dnn.parameters())\n",
    "        self.iter = 0\n",
    "        \n",
    "    def net_u(self, x, t):  \n",
    "        u = self.dnn(torch.cat([x, t], dim=1))\n",
    "        return u\n",
    "    \n",
    "    def net_f(self, x, t):\n",
    "        \"\"\" The pytorch autograd version of calculating residual \"\"\"\n",
    "        lambda_1 = self.lambda_1        \n",
    "        lambda_2 = torch.exp(self.lambda_2)\n",
    "        u = self.net_u(x, t)\n",
    "        \n",
    "        u_t = torch.autograd.grad(\n",
    "            u, t, \n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u_x = torch.autograd.grad(\n",
    "            u, x, \n",
    "            grad_outputs=torch.ones_like(u),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        u_xx = torch.autograd.grad(\n",
    "            u_x, x, \n",
    "            grad_outputs=torch.ones_like(u_x),\n",
    "            retain_graph=True,\n",
    "            create_graph=True\n",
    "        )[0]\n",
    "        \n",
    "        f = u_t + lambda_1 * u * u_x - lambda_2 * u_xx\n",
    "        return f\n",
    "    \n",
    "    def loss_func(self):\n",
    "        u_pred = self.net_u(self.x, self.t)\n",
    "        f_pred = self.net_f(self.x, self.t)\n",
    "        loss = torch.mean((self.u - u_pred) ** 2) + torch.mean(f_pred ** 2)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        self.iter += 1\n",
    "        if self.iter % 100 == 0:\n",
    "            print(\n",
    "                'Loss: %e, l1: %.5f, l2: %.5f' % \n",
    "                (\n",
    "                    loss.item(), \n",
    "                    self.lambda_1.item(), \n",
    "                    torch.exp(self.lambda_2.detach()).item()\n",
    "                )\n",
    "            )\n",
    "        return loss\n",
    "    \n",
    "    # def train(self, nIter):\n",
    "    #     self.dnn.train()\n",
    "    #     for epoch in range(nIter):\n",
    "    #         u_pred = self.net_u(self.x, self.t)\n",
    "    #         f_pred = self.net_f(self.x, self.t)\n",
    "    #         loss = torch.mean((self.u - u_pred) ** 2) + torch.mean(f_pred ** 2)\n",
    "            \n",
    "    #         # Backward and optimize\n",
    "    #         self.optimizer_Adam.zero_grad()\n",
    "    #         loss.backward()\n",
    "    #         self.optimizer_Adam.step()\n",
    "            \n",
    "    #         if epoch % 100 == 0:\n",
    "    #             print(\n",
    "    #                 'It: %d, Loss: %.3e, Lambda_1: %.3f, Lambda_2: %.6f' % \n",
    "    #                 (\n",
    "    #                     epoch, \n",
    "    #                     loss.item(), \n",
    "    #                     self.lambda_1.item(), \n",
    "    #                     torch.exp(self.lambda_2).item()\n",
    "    #                 )\n",
    "    #             )\n",
    "                \n",
    "    #     # Backward and optimize\n",
    "    #     self.optimizer.step(self.loss_func)\n",
    "    \n",
    "    def train(self, nIter):\n",
    "        self.dnn.train()\n",
    "        pbar = tqdm(range(nIter), desc=\"Training\", ncols=100)\n",
    "        for epoch in pbar:\n",
    "            u_pred = self.net_u(self.x, self.t)\n",
    "            f_pred = self.net_f(self.x, self.t)\n",
    "            loss = torch.mean((self.u - u_pred) ** 2) + torch.mean(f_pred ** 2)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            self.optimizer_Adam.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer_Adam.step()\n",
    "\n",
    "            # Update the progress bar with the loss information\n",
    "            pbar.set_postfix(\n",
    "                Loss=f\"{loss.item():.3e}\", \n",
    "                Lambda_1=f\"{self.lambda_1.item():.3f}\", \n",
    "                Lambda_2=f\"{torch.exp(self.lambda_2).item():.6f}\",\n",
    "                refresh=True\n",
    "            )\n",
    "                \n",
    "        # Backward and optimize\n",
    "        self.optimizer.step(self.loss_func)\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device)\n",
    "        t = torch.tensor(X[:, 1:2], requires_grad=True).float().to(device)\n",
    "\n",
    "        self.dnn.eval()\n",
    "        u = self.net_u(x, t)\n",
    "        f = self.net_f(x, t)\n",
    "        u = u.detach().cpu().numpy()\n",
    "        f = f.detach().cpu().numpy()\n",
    "        return u, f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu = 0.01/np.pi\n",
    "\n",
    "N_u = 2000\n",
    "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "\n",
    "data = scipy.io.loadmat('data/burgers_shock.mat')\n",
    "\n",
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = np.real(data['usol']).T\n",
    "\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = Exact.flatten()[:,None]              \n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.310215e-02, l1: 0.20404, l2: 0.00239\n",
      "Loss: 1.550285e-02, l1: 0.44315, l2: 0.00253\n",
      "Loss: 8.796914e-03, l1: 0.59975, l2: 0.00278\n",
      "Loss: 7.027099e-03, l1: 0.66012, l2: 0.00279\n",
      "Loss: 4.600182e-03, l1: 0.73827, l2: 0.00295\n",
      "Loss: 2.721438e-03, l1: 0.81310, l2: 0.00317\n",
      "Loss: 1.762194e-03, l1: 0.88132, l2: 0.00348\n",
      "Loss: 1.198461e-03, l1: 0.92897, l2: 0.00368\n",
      "Loss: 8.939077e-04, l1: 0.95892, l2: 0.00377\n",
      "Loss: 7.051424e-04, l1: 0.96341, l2: 0.00385\n",
      "Loss: 5.238830e-04, l1: 0.98313, l2: 0.00399\n",
      "Loss: 3.948458e-04, l1: 0.97661, l2: 0.00396\n",
      "Loss: 2.949358e-04, l1: 0.98505, l2: 0.00381\n",
      "Loss: 2.119915e-04, l1: 0.99167, l2: 0.00367\n",
      "Loss: 1.745338e-04, l1: 0.98633, l2: 0.00352\n",
      "Loss: 1.398610e-04, l1: 0.98692, l2: 0.00344\n",
      "Loss: 1.123266e-04, l1: 0.99513, l2: 0.00336\n",
      "Loss: 9.552659e-05, l1: 0.99338, l2: 0.00328\n",
      "Loss: 8.419155e-05, l1: 0.99151, l2: 0.00324\n",
      "Loss: 7.499390e-05, l1: 0.99365, l2: 0.00323\n",
      "Loss: 6.587169e-05, l1: 0.99272, l2: 0.00322\n",
      "Loss: 6.012571e-05, l1: 0.99300, l2: 0.00321\n",
      "Loss: 5.341434e-05, l1: 0.99538, l2: 0.00322\n",
      "Loss: 4.697083e-05, l1: 0.99658, l2: 0.00323\n",
      "Loss: 4.367676e-05, l1: 0.99506, l2: 0.00323\n",
      "Loss: 4.041976e-05, l1: 0.99468, l2: 0.00323\n",
      "Loss: 3.709706e-05, l1: 0.99799, l2: 0.00325\n",
      "Loss: 3.511725e-05, l1: 0.99781, l2: 0.00326\n",
      "Loss: 3.235144e-05, l1: 0.99848, l2: 0.00325\n",
      "Loss: 3.009527e-05, l1: 0.99836, l2: 0.00324\n",
      "Loss: 2.760351e-05, l1: 0.99851, l2: 0.00323\n",
      "Loss: 2.486373e-05, l1: 0.99816, l2: 0.00321\n",
      "Loss: 2.322572e-05, l1: 0.99826, l2: 0.00320\n",
      "Loss: 2.174522e-05, l1: 0.99865, l2: 0.00318\n",
      "Loss: 2.071332e-05, l1: 0.99877, l2: 0.00317\n",
      "Loss: 1.981033e-05, l1: 0.99934, l2: 0.00317\n",
      "Loss: 1.875369e-05, l1: 0.99920, l2: 0.00315\n",
      "Loss: 1.751824e-05, l1: 0.99794, l2: 0.00313\n",
      "Loss: 1.658522e-05, l1: 0.99910, l2: 0.00312\n",
      "Loss: 1.545949e-05, l1: 1.00018, l2: 0.00312\n",
      "Loss: 1.481121e-05, l1: 0.99918, l2: 0.00312\n",
      "Loss: 1.392621e-05, l1: 0.99959, l2: 0.00312\n",
      "Loss: 1.334907e-05, l1: 0.99908, l2: 0.00313\n",
      "Loss: 1.266841e-05, l1: 1.00027, l2: 0.00314\n",
      "CPU times: user 1min 24s, sys: 20.7 s, total: 1min 45s\n",
      "Wall time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "noise = 0.0\n",
    "\n",
    "# create training set\n",
    "idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n",
    "X_u_train = X_star[idx,:]\n",
    "u_train = u_star[idx,:]\n",
    "\n",
    "# training\n",
    "model = PhysicsInformedNN(X_u_train, u_train, layers, lb, ub)\n",
    "model.train(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PINNS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
